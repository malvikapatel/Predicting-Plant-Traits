{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfgIqn_AXxNd"
      },
      "source": [
        "Downloading Images and Data Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-MlHUpMNf5nU"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "!echo '{\"username\":\"malvikapatelll\",\"key\":\"006cf0f3c6830fca23fe7e0a63cd0f4c\"}' > kaggle.json\n",
        "!mkdir -p /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c cs-480-2024-spring\n",
        "!unzip *.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVZuuCLzeQvx"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d4cGAaGGXl5t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from PIL import Image\n",
        "from scipy.stats import randint\n",
        "from scipy.stats import zscore\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from transformers import ViTModel\n",
        "from xgboost import XGBRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Z1kKM7X2wf"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XFizwghmf_3w"
      },
      "outputs": [],
      "source": [
        "# Loading data from the csv files\n",
        "train_df = pd.read_csv('/content/data/train.csv')\n",
        "test_df = pd.read_csv('/content/data/test.csv')\n",
        "\n",
        "# Aligning columns of both the training dataset and the test dataset\n",
        "common_columns = train_df.columns[1:164].intersection(test_df.columns[1:164])\n",
        "\n",
        "# Separating the features and targets\n",
        "X_ancillary = train_df[common_columns].values\n",
        "y = train_df.iloc[:, 164:].values\n",
        "\n",
        "# Removing outliers\n",
        "z_scores = np.abs(zscore(X_ancillary))\n",
        "threshold = 3\n",
        "mask = (z_scores < threshold).all(axis=1)\n",
        "\n",
        "X_ancillary_filtered = X_ancillary[mask]\n",
        "y_filtered = y[mask]\n",
        "train_sample_filtered = train_df[mask].reset_index(drop=True)\n",
        "\n",
        "# Scaling to normalize data between 0 and 1\n",
        "# Scaling training set\n",
        "scaler = StandardScaler()\n",
        "X_ancillary_filtered = scaler.fit_transform(X_ancillary_filtered)\n",
        "X_test_ancillary = scaler.transform(test_df[common_columns].values)\n",
        "\n",
        "# Scaling test set\n",
        "target_scaler = StandardScaler()\n",
        "y_scaled_filtered = target_scaler.fit_transform(y_filtered)\n",
        "\n",
        "# Downsampling\n",
        "train_sample = train_sample_filtered.sample(frac=0.3, random_state=42)\n",
        "X_train_anc = X_ancillary_filtered[train_sample.index]\n",
        "y_train = y_scaled_filtered[train_sample.index]\n",
        "image_ids_train = train_sample_filtered['id'].values[train_sample.index]\n",
        "\n",
        "# Train-validation split\n",
        "X_train_img, X_val_img, X_train_anc, X_val_anc, y_train, y_val = train_test_split(\n",
        "    image_ids_train, X_train_anc, y_train, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BSxM8t3qFmze"
      },
      "outputs": [],
      "source": [
        "# Custom dataset class which inherits from PyTorch's Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_ids, ancillary_features, targets=None, img_dir=None, transform=None):\n",
        "        self.image_ids = image_ids\n",
        "        self.ancillary_features = ancillary_features\n",
        "        self.targets = targets\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.image_ids[idx]\n",
        "        img_path = os.path.join(self.img_dir, f\"{img_id}.jpeg\")\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        ancillary = torch.tensor(self.ancillary_features[idx], dtype=torch.float32)\n",
        "\n",
        "        if self.targets is not None:\n",
        "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
        "            return img, ancillary, target\n",
        "        else:\n",
        "            return img, ancillary\n",
        "\n",
        "# Data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Image directories\n",
        "train_img_dir = '/content/data/train_images'\n",
        "test_img_dir = '/content/data/test_images'\n",
        "\n",
        "# Datasets\n",
        "train_dataset = CustomDataset(\n",
        "    image_ids=X_train_img,\n",
        "    ancillary_features=X_train_anc,\n",
        "    targets=y_train,\n",
        "    img_dir=train_img_dir,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "val_dataset = CustomDataset(\n",
        "    image_ids=X_val_img,\n",
        "    ancillary_features=X_val_anc,\n",
        "    targets=y_val,\n",
        "    img_dir=train_img_dir,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = CustomDataset(\n",
        "    image_ids=test_df['id'].values,\n",
        "    ancillary_features=X_test_ancillary,\n",
        "    targets=None,\n",
        "    img_dir=test_img_dir,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8, prefetch_factor=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=8, prefetch_factor=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=8, prefetch_factor=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oS7WXNRBEMa",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Vision Transformer model\n",
        "class ViTForRegression(nn.Module):\n",
        "    def __init__(self, img_feature_dim, anc_feature_dim, hidden_dim, num_classes):\n",
        "        super(ViTForRegression, self).__init__()\n",
        "        # Pre-trained Vision Transformer model from Hugging Face\n",
        "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.img_embedding = nn.Linear(self.vit.config.hidden_size, hidden_dim)\n",
        "        self.anc_embedding = nn.Linear(anc_feature_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, img, anc_features):\n",
        "        # Forward pass\n",
        "        outputs = self.vit(pixel_values=img)\n",
        "        vit_outputs = outputs.last_hidden_state\n",
        "        img_emb = self.img_embedding(vit_outputs.mean(dim=1))\n",
        "\n",
        "        # Process ancillary features\n",
        "        anc_emb = self.anc_embedding(anc_features)\n",
        "        combined = torch.cat((img_emb, anc_emb), dim=1)\n",
        "        return self.fc(combined)\n",
        "\n",
        "# Initializing model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "hidden_dim = 256\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "model = ViTForRegression(768, X_train_anc.shape[1], hidden_dim, num_classes).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Gradscaler for Mixed Precision Training\n",
        "gradscaler = GradScaler()\n",
        "\n",
        "def train_model(model, dataloader, criterion, optimizer, device, gradscaler, accumulation_steps=2):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step, (images, anc_features, targets) in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
        "        images, anc_features, targets = images.to(device), anc_features.to(device), targets.to(device)\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(images, anc_features)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        gradscaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            gradscaler.step(optimizer)\n",
        "            gradscaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "# Training model\n",
        "num_train_epochs = 10\n",
        "for epoch in range(num_train_epochs):\n",
        "    train_loss = train_model(model, train_loader, criterion, optimizer, device, gradscaler, accumulation_steps=4)\n",
        "    print(f\"Epoch {epoch+1}/{num_train_epochs}, Loss: {train_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3VLgGCLKE6R3"
      },
      "outputs": [],
      "source": [
        "# Predicting  on test set\n",
        "def predict_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for images, anc_features in tqdm(dataloader, desc=\"Predicting\"):\n",
        "            images, anc_features = images.to(device), anc_features.to(device)\n",
        "            outputs = model(images, anc_features)\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "    return np.array(predictions)\n",
        "\n",
        "# RandomForestRegressor and XGBRegressor\n",
        "rf_regressor = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    max_features='sqrt',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_regressor = XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    tree_method='hist',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    early_stopping_rounds=10,\n",
        ")\n",
        "\n",
        "# Training the regression models\n",
        "rf_regressor.fit(X_train_anc, y_train)\n",
        "xgb_regressor.fit(X_train_anc, y_train, eval_set=[(X_val_anc, y_val)], verbose=False)\n",
        "\n",
        "# RandomForestRegressor Randomized Search\n",
        "rf_param_dist = {\n",
        "    'n_estimators': randint(100, 200),\n",
        "    'max_depth': randint(10, 20),\n",
        "    'max_features': ['auto', 'sqrt'],\n",
        "    'min_samples_split': randint(2, 5),\n",
        "    'min_samples_leaf': randint(1, 2),\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    rf_regressor, param_distributions=rf_param_dist, n_iter=10, cv=2, random_state=42, n_jobs=-1, scoring='r2'\n",
        ")\n",
        "rf_random_search.fit(X_train_anc, y_train)\n",
        "\n",
        "# XGBRegressor Randomized Search\n",
        "xgb_param_dist = {\n",
        "    'n_estimators': randint(100, 200),\n",
        "    'max_depth': randint(4, 10),\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "}\n",
        "\n",
        "xgb_regressor = XGBRegressor(objective='reg:squarederror', n_jobs=-1)\n",
        "\n",
        "xgb_random_search = RandomizedSearchCV(\n",
        "    xgb_regressor, param_distributions=xgb_param_dist, n_iter=10, cv=3, random_state=42, n_jobs=-1, scoring='r2'\n",
        ")\n",
        "xgb_random_search.fit(X_train_anc, y_train, eval_set=[(X_val_anc, y_val)])\n",
        "\n",
        "# Getting the best models\n",
        "best_rf = rf_random_search.best_estimator_\n",
        "best_xgb = xgb_random_search.best_estimator_\n",
        "\n",
        "# Re-training the best models\n",
        "best_rf.fit(X_train_anc, y_train)\n",
        "best_xgb.fit(X_train_anc, y_train, eval_set=[(X_val_anc, y_val)], verbose=False)\n",
        "\n",
        "# Combining predictions\n",
        "cnn_predictions_scaled = predict_model(model, test_loader, device)\n",
        "rf_predictions = best_rf.predict(X_test_ancillary)\n",
        "xgb_predictions = best_xgb.predict(X_test_ancillary)\n",
        "\n",
        "# Getting final predictions based on weightage\n",
        "y_test_pred = (0.5 * cnn_predictions_scaled + 0.25 * rf_predictions + 0.25 * xgb_predictions)\n",
        "y_test_pred_unscaled = target_scaler.inverse_transform(y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v8DlVJ6OYd9r"
      },
      "outputs": [],
      "source": [
        "# Creating submission file\n",
        "submission_df = pd.DataFrame(y_test_pred_unscaled, columns=['X4', 'X11', 'X18', 'X26', 'X50', 'X3112'])\n",
        "submission_df['id'] = test_df['id']\n",
        "submission_df = submission_df[['id', 'X4', 'X11', 'X18', 'X26', 'X50', 'X3112']]\n",
        "submission_df.to_csv('CS480ProjectSubmission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}